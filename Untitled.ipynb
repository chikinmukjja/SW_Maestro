{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['x'] = range(-10,10)\n",
    "train_df['y'] = train_df['x']*train_df['x']\n",
    "train_df['xx'] = train_df['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_df = pd.concat([train_df,train_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20480 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      "x     20480 non-null int32\n",
      "y     20480 non-null int32\n",
      "xx    20480 non-null int32\n",
      "dtypes: int32(3)\n",
      "memory usage: 400.0 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(alpha=1e-5,\n",
    "                    hidden_layer_sizes=(60,60), random_state=1, \n",
    "                    verbose=True, max_iter=10000,tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>xx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10</td>\n",
       "      <td>100</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9</td>\n",
       "      <td>81</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8</td>\n",
       "      <td>64</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7</td>\n",
       "      <td>49</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6</td>\n",
       "      <td>36</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5</td>\n",
       "      <td>25</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4</td>\n",
       "      <td>16</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3</td>\n",
       "      <td>9</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x    y  xx\n",
       "0 -10  100 -10\n",
       "1  -9   81  -9\n",
       "2  -8   64  -8\n",
       "3  -7   49  -7\n",
       "4  -6   36  -6\n",
       "5  -5   25  -5\n",
       "6  -4   16  -4\n",
       "7  -3    9  -3\n",
       "8  -2    4  -2\n",
       "9  -1    1  -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14199787\n",
      "Iteration 2, loss = 1.57930224\n",
      "Iteration 3, loss = 1.14322779\n",
      "Iteration 4, loss = 0.85250396\n",
      "Iteration 5, loss = 0.65448233\n",
      "Iteration 6, loss = 0.51744566\n",
      "Iteration 7, loss = 0.41590144\n",
      "Iteration 8, loss = 0.33422238\n",
      "Iteration 9, loss = 0.27167190\n",
      "Iteration 10, loss = 0.21868102\n",
      "Iteration 11, loss = 0.17741348\n",
      "Iteration 12, loss = 0.14418585\n",
      "Iteration 13, loss = 0.11647606\n",
      "Iteration 14, loss = 0.09507652\n",
      "Iteration 15, loss = 0.07752209\n",
      "Iteration 16, loss = 0.06388996\n",
      "Iteration 17, loss = 0.05263842\n",
      "Iteration 18, loss = 0.04371226\n",
      "Iteration 19, loss = 0.03670991\n",
      "Iteration 20, loss = 0.03083871\n",
      "Iteration 21, loss = 0.02607100\n",
      "Iteration 22, loss = 0.02211598\n",
      "Iteration 23, loss = 0.01899035\n",
      "Iteration 24, loss = 0.01637681\n",
      "Iteration 25, loss = 0.01428586\n",
      "Iteration 26, loss = 0.01236206\n",
      "Iteration 27, loss = 0.01087281\n",
      "Iteration 28, loss = 0.00956170\n",
      "Iteration 29, loss = 0.00841762\n",
      "Iteration 30, loss = 0.00747717\n",
      "Iteration 31, loss = 0.00665301\n",
      "Iteration 32, loss = 0.00596767\n",
      "Iteration 33, loss = 0.00533093\n",
      "Iteration 34, loss = 0.00478973\n",
      "Iteration 35, loss = 0.00432559\n",
      "Iteration 36, loss = 0.00391490\n",
      "Iteration 37, loss = 0.00353031\n",
      "Iteration 38, loss = 0.00320637\n",
      "Iteration 39, loss = 0.00291745\n",
      "Iteration 40, loss = 0.00265058\n",
      "Iteration 41, loss = 0.00242069\n",
      "Iteration 42, loss = 0.00220542\n",
      "Iteration 43, loss = 0.00201826\n",
      "Iteration 44, loss = 0.00185219\n",
      "Iteration 45, loss = 0.00169450\n",
      "Iteration 46, loss = 0.00155646\n",
      "Iteration 47, loss = 0.00143617\n",
      "Iteration 48, loss = 0.00132376\n",
      "Iteration 49, loss = 0.00122210\n",
      "Iteration 50, loss = 0.00113165\n",
      "Iteration 51, loss = 0.00104566\n",
      "Iteration 52, loss = 0.00096778\n",
      "Iteration 53, loss = 0.00089687\n",
      "Iteration 54, loss = 0.00083156\n",
      "Iteration 55, loss = 0.00077435\n",
      "Iteration 56, loss = 0.00071904\n",
      "Iteration 57, loss = 0.00066895\n",
      "Iteration 58, loss = 0.00062168\n",
      "Iteration 59, loss = 0.00057897\n",
      "Iteration 60, loss = 0.00054107\n",
      "Iteration 61, loss = 0.00050333\n",
      "Iteration 62, loss = 0.00046972\n",
      "Iteration 63, loss = 0.00043887\n",
      "Iteration 64, loss = 0.00041083\n",
      "Iteration 65, loss = 0.00038330\n",
      "Iteration 66, loss = 0.00035814\n",
      "Iteration 67, loss = 0.00033528\n",
      "Iteration 68, loss = 0.00031395\n",
      "Iteration 69, loss = 0.00029466\n",
      "Iteration 70, loss = 0.00027643\n",
      "Iteration 71, loss = 0.00025921\n",
      "Iteration 72, loss = 0.00024386\n",
      "Iteration 73, loss = 0.00022856\n",
      "Iteration 74, loss = 0.00021461\n",
      "Iteration 75, loss = 0.00020210\n",
      "Iteration 76, loss = 0.00018982\n",
      "Iteration 77, loss = 0.00017915\n",
      "Iteration 78, loss = 0.00016897\n",
      "Iteration 79, loss = 0.00015944\n",
      "Iteration 80, loss = 0.00015038\n",
      "Iteration 81, loss = 0.00014179\n",
      "Iteration 82, loss = 0.00013401\n",
      "Iteration 83, loss = 0.00012685\n",
      "Iteration 84, loss = 0.00012021\n",
      "Iteration 85, loss = 0.00011390\n",
      "Iteration 86, loss = 0.00010776\n",
      "Iteration 87, loss = 0.00010237\n",
      "Iteration 88, loss = 0.00009729\n",
      "Iteration 89, loss = 0.00009233\n",
      "Iteration 90, loss = 0.00008781\n",
      "Iteration 91, loss = 0.00008349\n",
      "Iteration 92, loss = 0.00007947\n",
      "Iteration 93, loss = 0.00007577\n",
      "Iteration 94, loss = 0.00007225\n",
      "Iteration 95, loss = 0.00006899\n",
      "Iteration 96, loss = 0.00006596\n",
      "Iteration 97, loss = 0.00006310\n",
      "Iteration 98, loss = 0.00006029\n",
      "Iteration 99, loss = 0.00005778\n",
      "Iteration 100, loss = 0.00005552\n",
      "Iteration 101, loss = 0.00005315\n",
      "Iteration 102, loss = 0.00005096\n",
      "Iteration 103, loss = 0.00004901\n",
      "Iteration 104, loss = 0.00004708\n",
      "Iteration 105, loss = 0.00004534\n",
      "Iteration 106, loss = 0.00004370\n",
      "Iteration 107, loss = 0.00004217\n",
      "Iteration 108, loss = 0.00004079\n",
      "Iteration 109, loss = 0.00003938\n",
      "Iteration 110, loss = 0.00003815\n",
      "Iteration 111, loss = 0.00003696\n",
      "Iteration 112, loss = 0.00003585\n",
      "Iteration 113, loss = 0.00003484\n",
      "Iteration 114, loss = 0.00003386\n",
      "Iteration 115, loss = 0.00003294\n",
      "Iteration 116, loss = 0.00003206\n",
      "Iteration 117, loss = 0.00003126\n",
      "Iteration 118, loss = 0.00003052\n",
      "Iteration 119, loss = 0.00002979\n",
      "Iteration 120, loss = 0.00002910\n",
      "Iteration 121, loss = 0.00002849\n",
      "Iteration 122, loss = 0.00002787\n",
      "Iteration 123, loss = 0.00002731\n",
      "Iteration 124, loss = 0.00002676\n",
      "Iteration 125, loss = 0.00002626\n",
      "Iteration 126, loss = 0.00002579\n",
      "Iteration 127, loss = 0.00002532\n",
      "Iteration 128, loss = 0.00002490\n",
      "Iteration 129, loss = 0.00002451\n",
      "Iteration 130, loss = 0.00002412\n",
      "Iteration 131, loss = 0.00002376\n",
      "Iteration 132, loss = 0.00002342\n",
      "Iteration 133, loss = 0.00002310\n",
      "Iteration 134, loss = 0.00002281\n",
      "Iteration 135, loss = 0.00002252\n",
      "Iteration 136, loss = 0.00002225\n",
      "Iteration 137, loss = 0.00002200\n",
      "Iteration 138, loss = 0.00002175\n",
      "Iteration 139, loss = 0.00002152\n",
      "Iteration 140, loss = 0.00002128\n",
      "Iteration 141, loss = 0.00002104\n",
      "Iteration 142, loss = 0.00002082\n",
      "Iteration 143, loss = 0.00002061\n",
      "Iteration 144, loss = 0.00002041\n",
      "Iteration 145, loss = 0.00002023\n",
      "Iteration 146, loss = 0.00002006\n",
      "Iteration 147, loss = 0.00001989\n",
      "Iteration 148, loss = 0.00001974\n",
      "Iteration 149, loss = 0.00001959\n",
      "Iteration 150, loss = 0.00001945\n",
      "Iteration 151, loss = 0.00001931\n",
      "Iteration 152, loss = 0.00001917\n",
      "Iteration 153, loss = 0.00001905\n",
      "Iteration 154, loss = 0.00001892\n",
      "Iteration 155, loss = 0.00001879\n",
      "Iteration 156, loss = 0.00001867\n",
      "Iteration 157, loss = 0.00001855\n",
      "Iteration 158, loss = 0.00001843\n",
      "Iteration 159, loss = 0.00001832\n",
      "Iteration 160, loss = 0.00001821\n",
      "Iteration 161, loss = 0.00001810\n",
      "Iteration 162, loss = 0.00001799\n",
      "Iteration 163, loss = 0.00001788\n",
      "Iteration 164, loss = 0.00001778\n",
      "Iteration 165, loss = 0.00001767\n",
      "Iteration 166, loss = 0.00001757\n",
      "Iteration 167, loss = 0.00001746\n",
      "Iteration 168, loss = 0.00001736\n",
      "Iteration 169, loss = 0.00001726\n",
      "Iteration 170, loss = 0.00001714\n",
      "Iteration 171, loss = 0.00001702\n",
      "Iteration 172, loss = 0.00001691\n",
      "Iteration 173, loss = 0.00001680\n",
      "Iteration 174, loss = 0.00001669\n",
      "Iteration 175, loss = 0.00001658\n",
      "Iteration 176, loss = 0.00001647\n",
      "Iteration 177, loss = 0.00001635\n",
      "Iteration 178, loss = 0.00001623\n",
      "Iteration 179, loss = 0.00001612\n",
      "Iteration 180, loss = 0.00001601\n",
      "Iteration 181, loss = 0.00001590\n",
      "Iteration 182, loss = 0.00001579\n",
      "Iteration 183, loss = 0.00001567\n",
      "Iteration 184, loss = 0.00001556\n",
      "Iteration 185, loss = 0.00001544\n",
      "Iteration 186, loss = 0.00001532\n",
      "Iteration 187, loss = 0.00001520\n",
      "Iteration 188, loss = 0.00001509\n",
      "Iteration 189, loss = 0.00001497\n",
      "Iteration 190, loss = 0.00001485\n",
      "Iteration 191, loss = 0.00001472\n",
      "Iteration 192, loss = 0.00001459\n",
      "Iteration 193, loss = 0.00001445\n",
      "Iteration 194, loss = 0.00001431\n",
      "Iteration 195, loss = 0.00001419\n",
      "Iteration 196, loss = 0.00001406\n",
      "Iteration 197, loss = 0.00001393\n",
      "Iteration 198, loss = 0.00001381\n",
      "Iteration 199, loss = 0.00001368\n",
      "Iteration 200, loss = 0.00001355\n",
      "Iteration 201, loss = 0.00001342\n",
      "Iteration 202, loss = 0.00001329\n",
      "Iteration 203, loss = 0.00001317\n",
      "Iteration 204, loss = 0.00001303\n",
      "Iteration 205, loss = 0.00001290\n",
      "Iteration 206, loss = 0.00001277\n",
      "Iteration 207, loss = 0.00001264\n",
      "Iteration 208, loss = 0.00001250\n",
      "Iteration 209, loss = 0.00001237\n",
      "Iteration 210, loss = 0.00001224\n",
      "Iteration 211, loss = 0.00001210\n",
      "Iteration 212, loss = 0.00001197\n",
      "Iteration 213, loss = 0.00001184\n",
      "Iteration 214, loss = 0.00001172\n",
      "Iteration 215, loss = 0.00001159\n",
      "Iteration 216, loss = 0.00001147\n",
      "Iteration 217, loss = 0.00001134\n",
      "Iteration 218, loss = 0.00001122\n",
      "Iteration 219, loss = 0.00001109\n",
      "Iteration 220, loss = 0.00001098\n",
      "Iteration 221, loss = 0.00001086\n",
      "Iteration 222, loss = 0.00001075\n",
      "Iteration 223, loss = 0.00001063\n",
      "Iteration 224, loss = 0.00001052\n",
      "Iteration 225, loss = 0.00001041\n",
      "Iteration 226, loss = 0.00001030\n",
      "Iteration 227, loss = 0.00001020\n",
      "Iteration 228, loss = 0.00001009\n",
      "Iteration 229, loss = 0.00000999\n",
      "Iteration 230, loss = 0.00000989\n",
      "Iteration 231, loss = 0.00000979\n",
      "Iteration 232, loss = 0.00000969\n",
      "Iteration 233, loss = 0.00000960\n",
      "Iteration 234, loss = 0.00000951\n",
      "Iteration 235, loss = 0.00000941\n",
      "Iteration 236, loss = 0.00000932\n",
      "Iteration 237, loss = 0.00000924\n",
      "Iteration 238, loss = 0.00000915\n",
      "Iteration 239, loss = 0.00000906\n",
      "Iteration 240, loss = 0.00000898\n",
      "Iteration 241, loss = 0.00000890\n",
      "Iteration 242, loss = 0.00000882\n",
      "Iteration 243, loss = 0.00000875\n",
      "Iteration 244, loss = 0.00000867\n",
      "Iteration 245, loss = 0.00000859\n",
      "Iteration 246, loss = 0.00000853\n",
      "Iteration 247, loss = 0.00000845\n",
      "Iteration 248, loss = 0.00000838\n",
      "Iteration 249, loss = 0.00000831\n",
      "Iteration 250, loss = 0.00000825\n",
      "Iteration 251, loss = 0.00000819\n",
      "Iteration 252, loss = 0.00000812\n",
      "Iteration 253, loss = 0.00000804\n",
      "Iteration 254, loss = 0.00000799\n",
      "Iteration 255, loss = 0.00000794\n",
      "Iteration 256, loss = 0.00000787\n",
      "Iteration 257, loss = 0.00000782\n",
      "Iteration 258, loss = 0.00000776\n",
      "Iteration 259, loss = 0.00000771\n",
      "Iteration 260, loss = 0.00000766\n",
      "Iteration 261, loss = 0.00000760\n",
      "Iteration 262, loss = 0.00000756\n",
      "Iteration 263, loss = 0.00000751\n",
      "Iteration 264, loss = 0.01039765\n",
      "Iteration 265, loss = 0.00001547\n",
      "Iteration 266, loss = 0.00001020\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(np.array(train_df.drop('y',axis=1)), np.array(train_df['y']))\n",
    "result = clf.predict(train_df.drop('y',axis=1))\n",
    "accuracy_score(train_df['y'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['result'] =  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14d0ab6e550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14d0ab65208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.head(20)['result'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
